# -*- coding: utf-8 -*-
"""Bank_prob.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a4tEh3m3GrrvGP3-XmJEehL-JHCgtNJy

# Data Processing

##data pre processing
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv('/content/churn.csv')

dataset

"""## Data Exploration"""

dataset.shape

dataset.info()

dataset.select_dtypes(include='object').columns

len(dataset.select_dtypes(include='object').columns)

dataset.describe()

dataset.isnull().values.sum()

dataset.isnull().values.any()
#dataset has no missing values



"""## Categorical data"""

#so our categorical values are objects
dataset.select_dtypes(include='object').columns
#but our target variable is the exited

dataset.head()

dataset = dataset.drop(columns = ['RowNumber', 'CustomerId', 'Surname'])
#we are removing the first 3 coz those are not important

dataset

dataset.select_dtypes('object').columns

#now there are 2 objects and now we are looking what are the possible answers on those geography and gender
dataset['Geography'].unique()

dataset['Gender'].unique()

dataset.groupby('Geography').mean()

dataset.groupby('Gender').mean()

# one hot encoding

dataset = pd.get_dummies(data=dataset, drop_first=True)
#we added the arguement of drop first so france and female wont be there on the new dataset means zero

dataset.head()

"""##Count Plot"""

sns.countplot(dataset['Exited'])
plt.plot

(dataset.Exited==0).sum()
#stayed with the costumers

(dataset.Exited==1).sum()
#left the bank

"""##Correlation matrix and heat map

"""

dataset_2 = dataset.drop(columns='Exited')

dataset_2.corrwith(dataset['Exited']).plot.bar(
    figsize=(16,9), title='Correlated with Exited', rot=45, grid= True
)

corr = dataset.corr()

plt.figure(figsize=(16,9))
sns.heatmap(corr, annot=True)

"""##Splitting the dataset"""

dataset.head()

#independent / matrix of features variables

x = dataset.drop(columns='Exited')
#selecting all columns but exited

#target Variable / dependent Variable
y = dataset['Exited']
#parang magiging y lang ung exited then ung iba sa X parang tinangal lang ung exited the x axis tas ginawang Y

#splitting dataset
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=0)

x_train

x_test

y_train



x_train.shape

x_test.shape

y_test.shape

y_train.shape

"""##Feature Scaling"""

#parang is sscale lang ng 0. something ung mga number pra nde ganun kalaki ung agwat sa isat isa
#magiging decimal ung scaling ng pag train pra nde malaki agwat

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

x_train

x_test

"""#Part 2 Building the model

##Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
classifier_lr = LogisticRegression(random_state=0)
classifier_lr.fit(x_train, y_train)

y_pred = classifier_lr.predict(x_test)
#dito prang i ppredict mo ung value ng y sa x test na binigay mo kasi ung classifier
#x_train na un kaya ttry mo sa test pra malaman kung mejo tumutugma ba ung prediction mo
#or may mistake

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
prec = precision_score(y_test,y_pred)
rec = recall_score(y_test, y_pred)

#dito prang kinokomapara ung y sa isang y titignan kung gaano ka accurate

results = pd.DataFrame([['Logistic regression', acc, f1, prec, rec]],
                       columns=['Model', 'Accuracy', 'F1', 'Precission', 'Recall'])

results
#we can say that we have a great accuracy its like 81 percent

#try natin ung confusion matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)

#may maling 96 sa upper right
#tas may maling 309 sa lower right
# so may tama sa taas na 1526 at sa baba na 96 wews baba sa baba

"""##cross validation"""

#kasama parint to sa logistic regression
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator=classifier_lr, X=x_train, y=y_train, cv=10)

print("Accuracy is {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation is {:.2f} %".format(accuracies.std()*100))



"""##Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

classifier_rf = RandomForestClassifier(random_state=0)
classifier_rf.fit(x_train, y_train)

from pprint import pprint
print('Parameters currently in use:\n')
pprint(classifier_rf.get_params())

y_pred = classifier_rf.predict(x_test)  
#katulad kanina pro random forest gamit ppredict ung value ng y sa x test

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
prec = precision_score(y_test,y_pred)
rec = recall_score(y_test, y_pred)

model_results = pd.DataFrame([['Random Forest', acc, f1, prec, rec]],
                       columns=['Model', 'Accuracy', 'F1', 'Precission', 'Recall'])

model_results

results.append(model_results, ignore_index=True)

results = results.append(model_results, ignore_index=True)

results

cm = confusion_matrix(y_test, y_pred)
print(cm)

#tas try ulit natin sa confusion matrix then cross validation din

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator=classifier_rf, X=x_train, y=y_train, cv=10)

print("Accuracy is {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation is {:.2f} %".format(accuracies.std()*100))

#diba mas malaki mas gumagana or masmalaki ung success rate sa random forest kesa sa logistic regression so 
#ganun nga prang 2 ways ung building model pipili ka lang kung anu tlga ung gumagana

"""##XGBOOST"""

#prang papabilisin lang neto ung algorithm 
#https://xgboost.readthedocs.io/en/stable/

from xgboost import XGBClassifier
classifier_xgb = XGBClassifier()
classifier_xgb.fit(x_train, y_train)

#babaguhin nanaman natin ung ypred value hahaha
y_pred = classifier_xgb.predict(x_test)

from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
prec = precision_score(y_test,y_pred)
rec = recall_score(y_test, y_pred)

model_results = pd.DataFrame([['XGBoost', acc, f1, prec, rec]],
                       columns=['Model', 'Accuracy', 'F1', 'Precission', 'Recall'])

results = results.append(model_results, ignore_index=True)
results

#masmataas prin ung random forest classifier kesa sa dalawa
cm = confusion_matrix(y_test, y_pred)
print(cm)

"""###cross validations of this one

"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator=classifier_xgb, X=x_train, y=y_train, cv=10)

print("Accuracy is {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation is {:.2f} %".format(accuracies.std()*100))



"""# Part 3 Randomize Search XGboost Classifier"""

#well ung sa knya mataas ung XGboost pro saking ung Random forest ung mataas LOL
#gagamit tayo ng randomize search pra malaman kung anong magandang parameters sa classifier na ito

results

#GRIDSEARCHCV TTRY NYAN LAHAT NG PARAMETERS PRO SA RANDOMIZESEARCHCV NDE RANDOM LANG TLGA
#BOBO ampota

from sklearn.model_selection import RandomizedSearchCV

parameters = {
    'learning_rate':[0.5, 0.1, 0.15, 0.20, 0.25, 0.30],
    'max_depth':[3, 4, 5, 6, 7, 8, 10, 12, 15],
    'min_child_weight':[1, 3, 5, 7],
    'gamma':[0.0, 0.1, 0.2, 0.3, 0.4],
    'colsample_bytree':[0.3, 0.4, 0.5, 0.7]
}

parameters

randomized_search = RandomizedSearchCV(estimator=classifier_xgb, param_distributions=parameters,
                                       n_iter=5, n_jobs=-1, scoring='roc_auc', cv=5, verbose=3)

randomized_search.fit(x_train, y_train)

randomized_search.best_estimator_

randomized_search.best_estimator_.get_params()
#para kita lahat nde ung ... sa huli hahah

randomized_search.best_params_

randomized_search.best_score_
#we can say 85 percent lol



"""#Part 4 final Model XGBoost Classifier"""

#dto final na ung model kaya lalagay natin ung best estimator ng model
#tas ung best params nya

from xgboost import XGBClassifier
#classifier_xgb = XGBClassifier()
#classifier_xgb.fit(x_train, y_train)

classifier = XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.4, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=0.2, gpu_id=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.15, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=4, max_leaves=None,
              min_child_weight=1, monotone_constraints=None,
              n_estimators=100, n_jobs=None, num_parallel_tree=None,
              predictor=None, random_state=None)
classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)
#tinangal ko ung arguement Missing= NONE pra gumana taenang yan

acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
prec = precision_score(y_test,y_pred)
rec = recall_score(y_test, y_pred)


model_results = pd.DataFrame([['Final XGBoost', acc, f1, prec, rec]],
                       columns=['Model', 'Accuracy', 'F1', 'Precission', 'Recall'])

results = results.append(model_results, ignore_index=True)
results

cm = confusion_matrix(y_test, y_pred)
print(cm)

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator=classifier, X=x_train, y=y_train, cv=10)

print("Accuracy is {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation is {:.2f} %".format(accuracies.std()*100))

"""#Part 5 Predicting a Single Observation"""

dataset.head()

#0 female 1 for male

single_obs = [[625, 45, 5, 12500.01, 1, 0, 1, 101348.88, 0, 0, 1]]

single_obs

classifier.predict(sc.transform(single_obs))

#array = 0 means theyre staying with us 1 means leave

